{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f1ec37",
   "metadata": {},
   "source": [
    "## Task\n",
    "Analyse this CSV file and clean the data, visualize data, and do preprocessing for machine learning.\n",
    "\n",
    "Here is all the data you need:\n",
    "\"/tmp/neuropsy_bp.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c074b7",
   "metadata": {},
   "source": [
    "## Plan\n",
    "* **data_loading**: Load the data from \"/tmp/neuropsy_bp.csv\" into a pandas DataFrame.\n",
    "* **data_exploration**: Explore the data to understand its structure, identify missing values, data types, and potential outliers.  Determine the shape of the data and the distribution of key variables.\n",
    "* **data_cleaning**: Handle missing values (e.g., imputation or removal), address outliers, and correct inconsistencies in the data.  Ensure data types are appropriate for analysis and modeling.\n",
    "* **data_wrangling**: Transform the data into a suitable format for machine learning. This may include encoding categorical variables, scaling numerical features, and creating new features if necessary.\n",
    "* **data_visualization**: Create visualizations (histograms, box plots, scatter plots, etc.) to explore the relationships between variables and gain insights into the data. Visualize the distribution of key variables before and after cleaning.\n",
    "* **feature_engineering**: Based on the data exploration and visualization, engineer new features that might improve model performance.  This could include interactions between existing variables, polynomial features, or other transformations.\n",
    "* **data_splitting**: Split the cleaned and preprocessed data into training, validation, and testing sets.\n",
    "* **data_preparation**: Finalize the data preparation steps, ensuring the data is ready for model training.\n",
    "* **finish_task**: Summarize the data cleaning, preprocessing, and visualization steps performed. Include key insights from the data exploration and visualizations.  Mention the prepared datasets (train, validation, test) and their locations (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba49a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from typing import Tuple, Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a6edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df: pd.DataFrame, impute: bool = False) -> pd.DataFrame:\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "\n",
    "    # Identify columns with more than 90% missing values\n",
    "    columns_to_drop = missing_percentage[missing_percentage > 90].index\n",
    "\n",
    "    # Drop the columns with more than 90% missing values\n",
    "    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    if impute:\n",
    "        # Impute missing numerical values with the median\n",
    "        numerical_cols = df_cleaned.select_dtypes(include=np.number).columns\n",
    "        for col in numerical_cols:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "\n",
    "        # Impute missing categorical values with the mode\n",
    "        categorical_cols = df_cleaned.select_dtypes(exclude=np.number).columns\n",
    "        for col in categorical_cols:\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1763c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(df, impute: bool = False, cat_cardinality_threshold: int = 5):\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    # Check cardinality for numerical columns, if < threshold, convert to categorical\n",
    "    move_to_cat = []\n",
    "    for col in numerical_cols:\n",
    "        if df[col].nunique() < cat_cardinality_threshold:\n",
    "            move_to_cat.append(col)\n",
    "    for col in move_to_cat:\n",
    "        numerical_cols.remove(col)\n",
    "        categorical_cols.append(col)\n",
    "\n",
    "    # Replace infinite values with NaN\n",
    "    df[numerical_cols] = df[numerical_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if impute:\n",
    "        # Impute NaN values in numerical columns with the median\n",
    "        for col in numerical_cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "        # Impute NaN values in categorical columns with the mode\n",
    "        for col in categorical_cols:\n",
    "            if df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Apply Label Encoding to categorical columns\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].astype(str)  # Convert to string to handle NaN as a category\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        sc = StandardScaler()\n",
    "        df[col] = sc.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e969b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from typing import Dict, Tuple, Any, Optional\n",
    "\n",
    "\n",
    "class DataWrangler:\n",
    "    \"\"\"\n",
    "    End-to-end dataframe pre-processor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conversion_threshold : float, default 0.80\n",
    "        Minimum share of non-null values that must convert to numeric\n",
    "        before an object column is treated as numeric.\n",
    "    cardinality_threshold : int, default 10\n",
    "        Numeric columns with < threshold distinct values are treated\n",
    "        as categoricals (avoids scaling one-hot-like/ordinal features).\n",
    "    scale_numeric : bool, default True\n",
    "        If True, apply `StandardScaler` to numeric columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        conversion_threshold: float = 0.80,\n",
    "        cardinality_threshold: int = 10,\n",
    "        scale_numeric: bool = True,\n",
    "    ) -> None:\n",
    "        self.conversion_threshold = conversion_threshold\n",
    "        self.cardinality_threshold = cardinality_threshold\n",
    "        self.scale_numeric = scale_numeric\n",
    "        self.encoders: Dict[str, LabelEncoder] = {}\n",
    "        self.scalers: Dict[str, StandardScaler] = {}\n",
    "        self.nan_medians: Dict[str, float] = {}\n",
    "\n",
    "    def _detect_and_cast(self, s: pd.Series) -> Tuple[pd.Series, bool]:\n",
    "        s_clean = s.replace([np.inf, -np.inf], np.nan)\n",
    "        if s_clean.dtype == \"object\":\n",
    "            s_clean = s_clean.str.strip()\n",
    "\n",
    "        coerced = pd.to_numeric(s_clean, errors=\"coerce\")\n",
    "        non_null, numeric_ok = s_clean.notna().sum(), coerced.notna().sum()\n",
    "\n",
    "        if non_null and numeric_ok / non_null >= self.conversion_threshold:\n",
    "            coerced = coerced.replace([np.inf, -np.inf], np.nan)\n",
    "            return coerced, True\n",
    "        return s_clean, False\n",
    "\n",
    "    def wrangle(\n",
    "        self, df: pd.DataFrame, *, fit: bool = True\n",
    "    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Transform *df* in-place-safe manner and return\n",
    "        (X_ready, params_dict) for downstream ML pipelines.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df   : input dataframe (left intact)\n",
    "        fit  : if False, assumes encoders/scalers are already fitted;\n",
    "               useful for transforming hold-out / test sets.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Numeric NaNs ⇒ median per column.\n",
    "        * Categorical NaNs ⇒ literal string ``\"Missing\"`` (is encoded).\n",
    "        * Encoders & scalers stored so ``inverse_transform`` is trivial.\n",
    "        \"\"\"\n",
    "        X = df.copy(deep=True)\n",
    "\n",
    "        for col in X.columns:\n",
    "            series, is_numeric = self._detect_and_cast(X[col])\n",
    "\n",
    "            # (1) treat as *categorical* (either truly object or low-cardinality numeric)\n",
    "            if (not is_numeric) or (\n",
    "                is_numeric and series.nunique(dropna=True) < self.cardinality_threshold\n",
    "            ):\n",
    "                # fill NaNs before encoding\n",
    "                series_filled = series.fillna(\"Missing\").astype(str)\n",
    "\n",
    "                if fit:\n",
    "                    le = LabelEncoder()\n",
    "                    X[col] = le.fit_transform(series_filled)\n",
    "                    self.encoders[col] = le\n",
    "                else:\n",
    "                    le = self.encoders[col]\n",
    "                    X[col] = le.transform(series_filled)\n",
    "\n",
    "            # (2) treat as *numeric*\n",
    "            else:\n",
    "                series = series.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "                # median from finite numbers only\n",
    "                if fit:\n",
    "                    finite = series.dropna()\n",
    "                    median = finite.median() if not finite.empty else 0.0\n",
    "                    self.nan_medians[col] = median\n",
    "                else:\n",
    "                    median = self.nan_medians[col]\n",
    "\n",
    "                series_filled = series.fillna(median).astype(float)\n",
    "\n",
    "                if self.scale_numeric:\n",
    "                    if fit:\n",
    "                        scaler = StandardScaler()\n",
    "                        # never passes NaN/Inf now\n",
    "                        X[col] = scaler.fit_transform(series_filled.values.reshape(-1, 1)).ravel()\n",
    "                        self.scalers[col] = scaler\n",
    "                    else:\n",
    "                        X[col] = self.scalers[col].transform(series_filled.values.reshape(-1, 1)).ravel()\n",
    "                else:\n",
    "                    X[col] = series_filled\n",
    "\n",
    "        params: Dict[str, Any] = {\n",
    "            \"encoders\": self.encoders,\n",
    "            \"scalers\": self.scalers,\n",
    "            \"nan_medians\": self.nan_medians,\n",
    "            \"scale_numeric\": self.scale_numeric,\n",
    "        }\n",
    "        return X, params\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 3.  Inverse utilities (optional convenience)\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def inverse_transform(\n",
    "        self, X: pd.DataFrame, drop_scaled: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Partially (or fully) reverse transformation.  Useful for:\n",
    "\n",
    "        * inspection of feature importances on original scale\n",
    "        * debugging / model explainability\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drop_scaled : if True, scaled numerics are restored to original values;\n",
    "                      otherwise, they stay standardised.\n",
    "        \"\"\"\n",
    "        rev = X.copy(deep=True)\n",
    "\n",
    "        # ▸ categorical\n",
    "        for col, le in self.encoders.items():\n",
    "            rev[col] = le.inverse_transform(rev[col].astype(int))\n",
    "\n",
    "        # ▸ numeric\n",
    "        if drop_scaled and self.scale_numeric:\n",
    "            for col, scaler in self.scalers.items():\n",
    "                col_arr = rev[col].values.reshape(-1, 1)\n",
    "                rev[col] = scaler.inverse_transform(col_arr).ravel()\n",
    "\n",
    "        # restore NaNs where appropriate\n",
    "        for col, median in self.nan_medians.items():\n",
    "            if pd.isna(median):\n",
    "                continue\n",
    "            rev.loc[rev[col] == median, col] = np.nan\n",
    "\n",
    "        return rev\n",
    "\n",
    "\n",
    "def wrangle(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    conversion_threshold: float = 0.80,\n",
    "    cardinality_threshold: int = 10,\n",
    "    scale_numeric: bool = True,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any], DataWrangler]:\n",
    "    \"\"\"\n",
    "    Stateless convenience wrapper; returns **(X_ready, params, fitted_wrangler)**.\n",
    "\n",
    "    You can later reuse ``fitted_wrangler.wrangle(new_df, fit=False)`` on\n",
    "    validation / test sets and call ``fitted_wrangler.inverse_transform``.\n",
    "\n",
    "    >>> X_train_ready, params, wr = wrangle(train_df)\n",
    "    >>> X_test_ready, _     = wr.wrangle(test_df, fit=False)\n",
    "    \"\"\"\n",
    "    wr = DataWrangler(\n",
    "        conversion_threshold=conversion_threshold,\n",
    "        cardinality_threshold=cardinality_threshold,\n",
    "        scale_numeric=scale_numeric,\n",
    "    )\n",
    "    X_ready, params = wr.wrangle(df, fit=True)\n",
    "    return X_ready, params, wr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64574ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m df_cleaned = clean(raw_data)\n\u001b[32m      7\u001b[39m df_cleaned.to_csv(\u001b[33m'\u001b[39m\u001b[33mFACE/neuropsy_bp_cleaned.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df_wrangled, params, wr = \u001b[43mwrangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversion_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcardinality_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m df_wrangled.to_csv(\u001b[33m'\u001b[39m\u001b[33mFACE/neuropsy_bp_wrangled.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 195\u001b[39m, in \u001b[36mwrangle\u001b[39m\u001b[34m(df, conversion_threshold, cardinality_threshold, scale_numeric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03mStateless convenience wrapper; returns **(X_ready, params, fitted_wrangler)**.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m \u001b[33;03m>>> X_test_ready, _     = wr.wrangle(test_df, fit=False)\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    190\u001b[39m wr = DataWrangler(\n\u001b[32m    191\u001b[39m     conversion_threshold=conversion_threshold,\n\u001b[32m    192\u001b[39m     cardinality_threshold=cardinality_threshold,\n\u001b[32m    193\u001b[39m     scale_numeric=scale_numeric,\n\u001b[32m    194\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m X_ready, params = \u001b[43mwr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X_ready, params, wr\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mDataWrangler.wrangle\u001b[39m\u001b[34m(self, df, fit)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fit:\n\u001b[32m    116\u001b[39m     scaler = StandardScaler()\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     X[col] = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries_filled\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.ravel()\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mself\u001b[39m.scalers[col] = scaler\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/base.py:918\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m         warnings.warn(\n\u001b[32m    904\u001b[39m             (\n\u001b[32m    905\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:894\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:930\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    899\u001b[39m \n\u001b[32m    900\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    927\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/chat-bot/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raw_data = pd.read_csv('FACE/neuropsy_bp.csv', sep=';', low_memory=False)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "df_cleaned = clean(raw_data)\n",
    "df_cleaned.to_csv('FACE/neuropsy_bp_cleaned.csv', index=False)\n",
    "\n",
    "\n",
    "df_wrangled, params, wr = wrangle(df_cleaned, conversion_threshold=0.80, cardinality_threshold=10, scale_numeric=True)\n",
    "df_wrangled.to_csv('FACE/neuropsy_bp_wrangled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce19b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55687a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5614ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa9018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b8814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61dbe101",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RANDOM_SAMPLES = 20\n",
    "CARDINALITY_THRESHOLD = 10      # below this distinct‐value count ⇒ “categorical”\n",
    "CONVERSION_THRESHOLD = 0.95     # ≥95% of non‐null ⇒ we’ll coerce to numeric\n",
    "\n",
    "def detect_and_cast(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Try to coerce object→numeric if most values convert cleanly.\n",
    "    Returns (cleaned_series, is_numeric_bool).\n",
    "    \"\"\"\n",
    "    s = series\n",
    "    # strip whitespace on object strings\n",
    "    if s.dtype == 'object':\n",
    "        s = s.str.strip()\n",
    "\n",
    "    # attempt numeric parse\n",
    "    coerced = pd.to_numeric(s, errors='coerce')\n",
    "    non_null = s.notna().sum()\n",
    "    numeric_ok = coerced.notna().sum()\n",
    "\n",
    "    # if enough values successfully parsed, treat as numeric\n",
    "    if non_null > 0 and (numeric_ok / non_null) >= CONVERSION_THRESHOLD:\n",
    "        return coerced, True\n",
    "    else:\n",
    "        return series, False\n",
    "\n",
    "\n",
    "with open('neuropsy_bp_log.txt', 'w') as f:\n",
    "    for col in df_cleaned.columns:\n",
    "        original = df_cleaned[col]\n",
    "        series, is_numeric = detect_and_cast(original)\n",
    "\n",
    "        # override: if it really only has a handful of distinct values,\n",
    "        # you might still want it “categorical”\n",
    "        if is_numeric and series.nunique() < CARDINALITY_THRESHOLD:\n",
    "            is_numeric = False\n",
    "\n",
    "        # Log the column name, whether it is numeric and provide 10 random samples\n",
    "        col_type = \"Numeric\" if is_numeric else \"Categorical\"\n",
    "        samples = series.sample(n=min(N_RANDOM_SAMPLES, len(series)), random_state=0).tolist()\n",
    "\n",
    "        f.write(f\"{col} -- {col_type} -- Samples: {samples}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e2ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4472   -0.67\n",
       "5759     NaN\n",
       "5752   -0.33\n",
       "1727   -1.00\n",
       "2908    0.00\n",
       "4135   -1.00\n",
       "550      NaN\n",
       "1758    1.33\n",
       "4077    0.67\n",
       "3029    0.00\n",
       "Name: mstcr_wais4, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[col].iloc[np.random.choice(df_cleaned.index, 10, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec4366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e49b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ef35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df_cleaned.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "numerical_cols = df_cleaned.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc60bbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cclin01_wais4',\n",
       " 'cclin02_wais4',\n",
       " 'cclin03_wais4',\n",
       " 'cclin04_wais4',\n",
       " 'cclin05_wais4',\n",
       " 'cclin06_wais4',\n",
       " 'cclin07_wais4',\n",
       " 'cclin08_wais4',\n",
       " 'cclin09_wais4',\n",
       " 'cclin10_wais4',\n",
       " 'cclin11_wais4',\n",
       " 'cclin17_wais4',\n",
       " 'trbapp01_wais4',\n",
       " 'trbapp02_wais4',\n",
       " 'trbapp03_wais4',\n",
       " 'trbapp04_wais4',\n",
       " 'trbapp05_wais4',\n",
       " 'trbapp06_wais4',\n",
       " 'trbapp07_wais4',\n",
       " 'trbapp08_wais4',\n",
       " 'trbapp09_wais4',\n",
       " 'trbapp10_wais4',\n",
       " 'vocabtot_wais4',\n",
       " 'cvlt05z_wais4',\n",
       " 'cvlt08z_wais4',\n",
       " 'cvlt09z_wais4',\n",
       " 'cvlt10z_wais4',\n",
       " 'cvlt11z_wais4',\n",
       " 'cvlt12_wais4',\n",
       " 'cvlt12z_wais4',\n",
       " 'cvlt13_wais4',\n",
       " 'cvlt13z_wais4',\n",
       " 'cvlt14z_wais4',\n",
       " 'cvlt15z_wais4',\n",
       " 'cvlt16_wais4',\n",
       " 'cvlt16z_wais4',\n",
       " 'cvlt17_wais4',\n",
       " 'cvlt17z_wais4',\n",
       " 'cvlt18_wais4',\n",
       " 'cvlt18z_wais4',\n",
       " 'cvlt19_wais4',\n",
       " 'cvlt20_wais4',\n",
       " 'cvlt20z_wais4',\n",
       " 'cvlt21_wais4',\n",
       " 'cvlt22_wais4',\n",
       " 'code01_wais4',\n",
       " 'symbol01_wais4',\n",
       " 'ivt02_wais4',\n",
       " 'ivt03_wais4',\n",
       " 'wais4_mcod_4',\n",
       " 'wais4_mcod_5',\n",
       " 'wais4_mcod_6',\n",
       " 'wais4_mcod_7',\n",
       " 'wais4_mcod_8',\n",
       " 'wais4_mcoi_3',\n",
       " 'wais4_mcoi_4',\n",
       " 'wais4_mcoi_5',\n",
       " 'wais4_mcoi_6',\n",
       " 'wais4_mcoi_7',\n",
       " 'mcodemp_wais4',\n",
       " 'nbrut_wais4',\n",
       " 'mcodemp_wais4.1',\n",
       " 'nbrut_wais4.1',\n",
       " 'tmta01_wais4',\n",
       " 'tmta01a_wais4',\n",
       " 'tmta02a_wais4',\n",
       " 'tmtb01_wais4',\n",
       " 'tmtb01a_wais4',\n",
       " 'tmtb03_wais4',\n",
       " 'tmtb04_wais4',\n",
       " 'tmtb02a_wais4',\n",
       " 'tmtb05_wais4',\n",
       " 'tmtb05a_wais4',\n",
       " 'tmtba01a_wais4',\n",
       " 'tmtba02a_wais4',\n",
       " 'stroob04_wais4',\n",
       " 'fv03_wais4',\n",
       " 'fv10_wais4',\n",
       " 'fv12_wais4',\n",
       " 'fv14_wais4',\n",
       " 'fv21_wais4',\n",
       " 'fv22_wais4',\n",
       " 'cpt201a_wais4',\n",
       " 'cpt201b_wais4',\n",
       " 'cpt201c_wais4',\n",
       " 'cpt201d_wais4',\n",
       " 'cpt201e_wais4',\n",
       " 'cpt202b_wais4',\n",
       " 'cpt202c_wais4',\n",
       " 'cpt202d_wais4',\n",
       " 'cpt202e_wais4',\n",
       " 'cpt203a_wais4',\n",
       " 'cpt203b_wais4',\n",
       " 'cpt203c_wais4',\n",
       " 'cpt203d_wais4',\n",
       " 'cpt204a_wais4',\n",
       " 'cpt204b_wais4',\n",
       " 'cpt204c_wais4',\n",
       " 'cpt204d_wais4',\n",
       " 'cpt205a_wais4',\n",
       " 'cpt205b_wais4',\n",
       " 'cpt205c_wais4',\n",
       " 'cpt205d_wais4',\n",
       " 'cpt206a_wais4',\n",
       " 'cpt206b_wais4',\n",
       " 'cpt206c_wais4',\n",
       " 'cpt206d_wais4',\n",
       " 'cpt207a_wais4',\n",
       " 'cpt207b_wais4',\n",
       " 'cpt207c_wais4',\n",
       " 'cpt207d_wais4',\n",
       " 'cpt208b_wais4',\n",
       " 'cpt208c_wais4',\n",
       " 'cpt208d_wais4',\n",
       " 'cpt208e_wais4',\n",
       " 'cpt209a_wais4',\n",
       " 'cpt209b_wais4',\n",
       " 'cpt209c_wais4',\n",
       " 'cpt209d_wais4',\n",
       " 'cpt210a_wais4',\n",
       " 'cpt210b_wais4',\n",
       " 'cpt210c_wais4',\n",
       " 'cpt210d_wais4',\n",
       " 'cpt211a_wais4',\n",
       " 'cpt211b_wais4',\n",
       " 'cpt211c_wais4',\n",
       " 'cpt211d_wais4',\n",
       " 'cpt212a_wais4',\n",
       " 'cpt212b_wais4',\n",
       " 'cpt212c_wais4',\n",
       " 'cpt212d_wais4']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507a6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       32\n",
       "1       33\n",
       "2       25\n",
       "3       59\n",
       "4       35\n",
       "        ..\n",
       "6418    47\n",
       "6419    72\n",
       "6420    51\n",
       "6421    35\n",
       "6422    24\n",
       "Name: cclin01_wais4, Length: 6423, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['cclin01_wais4']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bdbe4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94b249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54c86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ff5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object', 'bool']).columns\n",
    "numerical_cols = df_cleaned.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_cleaned[numerical_cols] = df_cleaned[numerical_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute NaN values in numerical columns with the median\n",
    "for col in numerical_cols:\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "\n",
    "# Create transformers\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "df_transformed = preprocessor.fit_transform(df_cleaned)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "if hasattr(df_transformed, 'toarray'):\n",
    "    df_transformed = df_transformed.toarray()\n",
    "\n",
    "# Get feature names correctly\n",
    "feature_names = list(numerical_cols)\n",
    "categorical_feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
    "feature_names.extend(categorical_feature_names)\n",
    "\n",
    "# Create the DataFrame\n",
    "df_wrangled = pd.DataFrame(df_transformed, columns=feature_names)\n",
    "# df_wrangled.to_csv('FACE/neuropsy_bp_wrangled-OneHot.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object', 'bool']).columns\n",
    "numerical_cols = df_cleaned.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_cleaned[numerical_cols] = df_cleaned[numerical_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute NaN values in numerical columns with the median\n",
    "for col in numerical_cols:\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "\n",
    "# Apply Label Encoding to categorical columns\n",
    "# Create a copy to avoid SettingWithCopyWarning\n",
    "df_encoded = df_cleaned.copy()\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Handle potential NaN values before encoding\n",
    "    df_encoded[col] = df_encoded[col].astype(str) # Convert to string to handle NaN as a category\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "\n",
    "# Now apply StandardScaler to numerical columns\n",
    "# Identify numerical columns again as LabelEncoding converts some to numerical types\n",
    "numerical_cols_after_encoding = df_encoded.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Create transformers for numerical columns\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "# Note: We only transform numerical columns now as categorical columns are already encoded\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols_after_encoding)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep the encoded categorical columns\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "df_transformed = preprocessor.fit_transform(df_encoded)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "df_wrangled = pd.DataFrame(df_transformed, columns=feature_names)\n",
    "\n",
    "df_wrangled.to_csv('FACE/neuropsy_bp_wrangled-lEncoder.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d647e7",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "### Subtask:\n",
    "Visualize the data to explore relationships between variables and gain insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labs_signature",
   "metadata": {},
   "source": [
    "An AI generated notebook created with [Data Science Agent](https://labs.google.com/code/dsa) from Google Labs ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAFI0lEQVRYCb1XbUxTVxh+z71tKZTvUVBGZAXKhzI6XJZFpiFziiZuM5M5A4vJnBo2FrdlcUtMNo1LDGbJEsOP7Y9uETYXFdgQp7E4EHEIEeXDBaEqBSYfc0gptEBv23t2zi0ll9sPWhN2/pz3PM973ue9573nnHsRiFoXHlNpGmFTxKxsO8zKCgAz4SL66U2EJ0DO3wUlf0ZfYL+4Ba2wuoMht9HfP5Ks6VKdBAfa5MaWpZfhq0bdk30pKSmDNL6QwMTDkVUxnapfgEd5yyIqDcrgFtML1qLY1MQhBmOsiukKrfzfxGky5EGpJtVGuN60AybZammSfsaYcAulk/j54ySuZBjl3CEDK97ryXhHvjGc+vNUXYUmOT3NbPnHJB96NBhDPVclJZsiEmLtA4a+qJLte/s/S3tvvfcIEnQWvy8DB5slgX0ONyStdRwfLIs0tHckQlQI5OXmPaDOLe3NaWC2QYw63rIuUef0GUBK2NlshM9PTRM84O1mZyxdisJEXUN1U9OrfG4+jdnIdDRtLMzP56pHuuR8uE6q42dsYQjJ+3HwoOzg5BCDQOtUy90ktRGLgHJuLMCepwnQFyfgNo6mbVgpgxnrzMI8wZazQLmAA7kcMU0gqBaKlSzMOcGGuYWVsxMbzzlA4IKKBhB0As1jtzngeXg2LEHh1loR5ipH43BbsCUIMgHE3S/8aGf+yjVpE7GsOtOdwDNsQnby2ox/d31clA+M7Z4bD6QPagUKftxjp0ENX/9hJF2USCC8/9C1UTp+5WSRTIQvaQacwE1LR1P973WrDx481BbOx7wojcyAKufo4bKWlstXtbcsnU1S3teYngOThBQ/jacv4gzo7bj0ldkpkyNHOmnSkZ5OLmRnXendC1XVGbaKvwcBK7S+/OZxc0ArkHe6mKUTug/X95HOpzj12bpx2yQ3M6soqC0J6ERcMoFmU9u1m3X61K+OlrXFIfXLVMRLcwzj4Zat+pKefUc+2AAKBuorqzPbp5cuhf8S0KUvjk/XZq0ZMXx5g5x8SO0pPte75eIBVl9To4Vp8o7StcLksuQxMJEKcP4w9hCwLNVznoD4L8Gb50s5IOfNt6XHyKUjFUcz5QM/3WAOaDL1p89qYUbYIAB04Yk4TYSf4mBX7acWH+IC7LMEreStrzt7LpueFMd/LU8C2dxf7kCPudHW16p2D3/yeel6PEY+78g9IAi7HWhPEyGlOFdZoevl7l8XU2LbewnQ3D35fk2aw2SVC0cVPXQ1UfDhG3tu9T0ZiGyoupABNgLKibB94UoQx3XZiPAYQ2RSnNV8oneU7Io0iZP3Erxz+Qu7IC6jtSRTyO0HRjN8X37ipYaff8sQlphi/sSpEhGnqzP1aFy1v/XYlERcGNISkEiL24PHA66t5ph/OlpTcaPCUkzMi22na26noTtaDM/biB6bHu9BXETsnOCgJJRYiMbySHc+lK+OKpCb2+kUrn6pF4NwldlIts1zYsaGpu/EleastoBV6UogWFV3NJIxKZWKD7EZv7t9R43i17kZoUfYiHCtqQ449vVFBB2g2Z6eaeN4CBvisUIevr4Aos/xDj4rIlkNWOn57Sl3XEJYb3oLzGyNrxjLikc7CxnYHK0Hlr++rELeglPNTdFXGISQFXQhu4H8LnnzWxaMahFNqi3UF6WGDkEuXwzkx3FZBMVBqUauoljQJPii15v+nudckW8DG/Mu2Jnnye6IFc99ahthE8j4Tgjjz3Rvtl/SiX7P/wMAnfFmGtKN6gAAAABJRU5ErkJggg==)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
